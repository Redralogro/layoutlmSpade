{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "import json\n",
    "import requests\n",
    "from transformers import LayoutLMModel\n",
    "from pytorch_lightning import LightningModule\n",
    "import torch.nn as nn\n",
    "from infer.model.loss import BboxLoss\n",
    "from infer.model.spade_model import RelationTagger\n",
    "import json\n",
    "\n",
    "jsonfile =  json.load(open('check.jsonl')) \n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/layoutlm-base-uncased\")\n",
    "def get_bbox(jsonl_file):\n",
    "    w = jsonl_file['img_sz']['width']\n",
    "    h = jsonl_file['img_sz']['height']\n",
    "    bboxes = [[ int(x[0][0]*1000/w),int(x[0][1]*1000/h) ,int(x[2][0]*1000/w),int(x[2][1]*1000/h)] for x in jsonl_file['coord']]\n",
    "    return  torch.tensor(bboxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "\n",
    "\n",
    "words = jsonfile['text']\n",
    "normalized_word_boxes = get_bbox(jsonfile)\n",
    "encoding = tokenizer(\" \".join(words), return_tensors='pt')\n",
    "token_boxes = []\n",
    "i = 0\n",
    "maps = []\n",
    "maps.extend([i])\n",
    "result = {}\n",
    "for word, box in zip(words, normalized_word_boxes):\n",
    "    word_tokens = tokenizer.tokenize(word)\n",
    "    token_boxes.extend([box] * len(word_tokens))\n",
    "    i += 1\n",
    "    maps.extend([i]*len(word_tokens))\n",
    "\n",
    "maps.extend([i+1])\n",
    "\n",
    "token_boxes = [[0, 0, 0, 0]] + token_boxes + [[1000, 1000, 1000, 1000]]\n",
    "\n",
    "input_ids = encoding[\"input_ids\"]\n",
    "attention_mask = encoding[\"attention_mask\"]\n",
    "token_type_ids = encoding[\"token_type_ids\"]\n",
    "bbox = torch.tensor([token_boxes])\n",
    "maps = torch.tensor(maps).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m = np.array(maps.cpu())\n",
    "# m = m.tolist()\n",
    "# box_index =  np.unique(m).tolist()\n",
    "# [[item]*m.count(item) for item in box_index]\n",
    "# input_ids.shape\n",
    "model = LayoutLMModel.from_pretrained(\n",
    "            \"microsoft/layoutlm-base-uncased\", local_files_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(input_ids=input_ids,bbox=bbox,attention_mask=attention_mask,token_type_ids=token_type_ids)\n",
    "outputs.last_hidden_state.shape\n",
    "\n",
    "reduce_size = 256\n",
    "\n",
    "ln = nn.Linear(model.config.hidden_size, reduce_size)\n",
    "\n",
    "last_hidden_state = ln(outputs.last_hidden_state)\n",
    "last_hidden_state.shape\n",
    "\n",
    "zeros_temp = torch.zeros(reduce_size)\n",
    "\n",
    "def tranfer_maps(maps: Tensor):\n",
    "    m = []\n",
    "    maps = torch.squeeze(maps, 0)\n",
    "    uniq, count= maps.unique(return_counts=True)\n",
    "    for i in range(len(count)):\n",
    "        m.append(torch.stack([uniq[i]]*count[i]))\n",
    "    return m\n",
    "\n",
    "def reduce_shape(last_hidden_state, maps):\n",
    "    i = 0\n",
    "    device = last_hidden_state.device\n",
    "    reduce = []\n",
    "    for j  in range(len(maps)):\n",
    "        g_token = maps[j]\n",
    "        ten = zeros_temp.to(device)\n",
    "        for k in range(g_token.shape[0]):\n",
    "            ten += last_hidden_state[0][i]\n",
    "            i += 1\n",
    "        ten = ten/g_token.shape[0]\n",
    "        reduce.append(ten)\n",
    "    reduce = torch.stack(reduce).to(device)\n",
    "    return reduce\n",
    "\n",
    "\n",
    "\n",
    "reduce = reduce_shape(last_hidden_state, tranfer_maps(maps))\n",
    "# # maps\n",
    "\n",
    "torch.mean(reduce -reduce)\n",
    "# tranfer_maps(maps)\n",
    "# maps = \n",
    "# reduce_shape(last_hidden_state, tranfer_maps(maps) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %reload_ext autoreload\n",
    "from models.layout_parsing import LitBaseParsing\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from models.base import  LitLayoutParsing\n",
    "model = LitBaseParsing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# totalGraph =  model(input_ids, attention_mask, token_type_ids, bbox,maps)\n",
    "\n",
    "# totalGraph.shape\n",
    "dummy_input = torch.zeros(3,126)\n",
    "dummy_input[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dummy_input = torch.zeros_like(input_ids).cuda()\n",
    "# dummy_boxes = torch.zeros_like(bbox).cuda()\n",
    "# dummy_maps = torch.zeros_like(maps).cuda()\n",
    "import torch\n",
    "dummy_input = torch.zeros(1,196).cuda()\n",
    "dummy_boxes = torch.zeros(1,196,4).cuda()\n",
    "dummy_maps = torch.zeros(1,196).cuda()\n",
    "# totalGraph[0].shape\n",
    "# S = totalGraph[0].unsqueeze(0)\n",
    "# G = totalGraph[1].unsqueeze(0)\n",
    "# dummy_maps.shape\n",
    "# dummy_maps.unsqueeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "providers = [\n",
    "            # 'TensorrtExecutionProvider',\n",
    "                     'CUDAExecutionProvider',\n",
    "                     'CPUExecutionProvider']\n",
    "\n",
    "ort_sess = ort.InferenceSession(\n",
    "                                path_or_bytes = 'resources/onnxLitLP_03-03-2023_15-45-29_fp32.onnx'\n",
    "                                , providers= providers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'input_2'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_name = ort_sess.get_inputs()\n",
    "# output_name = ort_sess.get_outputs()[0].name\n",
    "input_name[1].name\n",
    "\n",
    "\n",
    "# outputs = ort_sess.run(None, \n",
    "#                         {\n",
    "#                             'input_1': input_ids.numpy()\n",
    "#                             'input_2': input_ids.numpy()\n",
    "#                             'input_3': input_ids.numpy()\n",
    "#                             'input_4': input_ids.numpy()\n",
    "#                             'input_5': input_ids.numpy()\n",
    "#                         }\n",
    "#                         )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
