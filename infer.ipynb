{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "import json\n",
    "import requests\n",
    "from transformers import LayoutLMModel\n",
    "from pytorch_lightning import LightningModule\n",
    "import torch.nn as nn\n",
    "from infer.model.loss import BboxLoss\n",
    "from infer.model.spade_model import RelationTagger\n",
    "import json\n",
    "\n",
    "# jsonfile =  json.load(open('./notebook/json_file/cccd_test.jsonl')) \n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/layoutlm-base-uncased\")\n",
    "def get_bbox(jsonl_file):\n",
    "    w = jsonl_file['img_sz']['width']\n",
    "    h = jsonl_file['img_sz']['height']\n",
    "    bboxes = [[ int(x[0][0]*1000/w),int(x[0][1]*1000/h) ,int(x[2][0]*1000/w),int(x[2][1]*1000/h)] for x in jsonl_file['coord']]\n",
    "    # return  torch.tensor(bboxes)\n",
    "    return bboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(188, torch.Size([1, 188]))"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dataModel.datamd_v2 import DpDataSet\n",
    "import numpy as np\n",
    "data = DpDataSet('./notebook/json_file/cccd_test.jsonl')\n",
    "\n",
    "data[0]['maps_tensor'].shape, \n",
    "\n",
    "i = 0\n",
    "for item in data[0]['maps']:\n",
    "    for idx in item: \n",
    "        i = i + 1\n",
    "i, data[0]['maps_tensor'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m = np.array(maps.cpu())\n",
    "# m = m.tolist()\n",
    "# box_index =  np.unique(m).tolist()\n",
    "# [[item]*m.count(item) for item in box_index]\n",
    "# input_ids.shape\n",
    "from functools import lru_cache\n",
    "@lru_cache\n",
    "def load_model():\n",
    "\n",
    "    return LayoutLMModel.from_pretrained(\n",
    "                \"microsoft/layoutlm-base-uncased\", local_files_only=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/layoutlm-base-uncased were not used when initializing LayoutLMModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing LayoutLMModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LayoutLMModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from torch import Tensor\n",
    "model =  load_model()\n",
    "# maps\n",
    "input_ids = data[0]['input_ids']\n",
    "bbox = data[0]['bbox']\n",
    "attention_mask = data[0]['attention_mask']\n",
    "token_type_ids = data[0]['token_type_ids']\n",
    "maps = data[0]['maps']\n",
    "maps_tensor = data[0]['maps_tensor']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.2074,  0.4719, -0.0585,  ...,  0.0695, -0.2073, -0.5689],\n",
       "         [-0.1731,  0.2456, -0.0784,  ..., -0.1674,  0.1176, -0.3298],\n",
       "         [ 0.4754,  0.1427, -0.2165,  ...,  0.1982, -0.1680, -0.2255],\n",
       "         ...,\n",
       "         [ 0.2280,  0.2522, -0.2653,  ...,  0.1527,  0.2269,  0.0562],\n",
       "         [-0.0099,  0.4010, -0.3135,  ..., -0.1487,  0.1672, -0.1883],\n",
       "         [-0.0500,  0.2073,  0.3541,  ..., -0.3969, -0.1453, -0.2007]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = model(input_ids=input_ids,bbox=bbox,attention_mask=attention_mask,token_type_ids=token_type_ids)\n",
    "outputs.last_hidden_state.shape\n",
    "\n",
    "reduce_size = 256\n",
    "\n",
    "ln = nn.Linear(model.config.hidden_size, reduce_size)\n",
    "\n",
    "last_hidden_state = ln(outputs.last_hidden_state)\n",
    "last_hidden_state.shape\n",
    "\n",
    "zeros_temp = torch.zeros(reduce_size)\n",
    "\n",
    "last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def tranfer_maps(maps: Tensor):\n",
    "#     m = []\n",
    "#     maps = torch.squeeze(maps, 0)\n",
    "#     uniq, count= maps.unique(return_counts=True)\n",
    "#     for i in range(len(count)):\n",
    "#         m.append(torch.stack([uniq[i]]*count[i]))\n",
    "#     return m\n",
    "\n",
    "\n",
    "def reduce_shape_old(last_hidden_state, maps):\n",
    "        device = last_hidden_state.device\n",
    "        reduce = []\n",
    "        for g_token in maps:\n",
    "            ten = zeros_temp.to(device)\n",
    "            for ele in g_token:\n",
    "                # print(g_token[0],'-----')\n",
    "                ten += last_hidden_state[0][g_token[0]]\n",
    "                # print(ele,'----')\n",
    "            ten = ten/len(g_token)\n",
    "            # print(len(g_token))\n",
    "            reduce.append(ten)\n",
    "        reduce = torch.stack(reduce).to(device)\n",
    "        return reduce\n",
    "\n",
    "\n",
    "\n",
    "reduce_old = reduce_shape_old(last_hidden_state, maps)\n",
    "\n",
    "\n",
    "# tranfer_maps(maps)\n",
    "# maps = \n",
    "# reduce_shape(last_hidden_state, tranfer_maps(maps) ) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 8020.1729, -5490.8179,   162.1636,  ..., -4435.5386,  6646.5469,\n",
       "          5353.5430],\n",
       "        [ 4010.5117, -2745.9504,    81.3024,  ..., -2217.8000,  3323.6831,\n",
       "          2676.8198],\n",
       "        [ 8021.1714, -5492.0972,   162.6142,  ..., -4436.2832,  6647.3828,\n",
       "          5353.8818],\n",
       "        ...,\n",
       "        [ 8083.3086, -5533.5264,   163.7679,  ..., -4470.3042,  6698.4453,\n",
       "          5396.2227],\n",
       "        [ 8083.7129, -5533.8901,   163.7538,  ..., -4470.7573,  6698.7896,\n",
       "          5396.1689],\n",
       "        [ 8084.0513, -5534.0591,   163.5114,  ..., -4470.8013,  6698.9219,\n",
       "          5396.2764]], grad_fn=<StackBackward0>)"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def reduce_shape(last_hidden_state, maps: Tensor):\n",
    "    maps = torch.squeeze(maps, 0)\n",
    "    uniq, count= maps.unique(return_counts=True)\n",
    "    device = last_hidden_state.device\n",
    "    reduce = []\n",
    "    for idx,  g_token in enumerate (uniq):\n",
    "        ten = zeros_temp.to(device)\n",
    "        for j in range (count[idx]):\n",
    "            # print(g_token.numpy(),'-----')\n",
    "            ten += last_hidden_state[0][g_token.numpy()]\n",
    "        # print(g_token, idx)\n",
    "        ten = ten/count.detach().numpy()[idx]\n",
    "        # print(count.detach().numpy()[idx])\n",
    "        reduce.append(ten)\n",
    "    reduce = torch.stack(reduce).to(device)\n",
    "    return reduce\n",
    "reduce = reduce_shape(last_hidden_state, maps_tensor)\n",
    "# # maps\n",
    "\n",
    "# with torch.no_grad():\n",
    "#   b =  torch.mean(reduce -reduce_old)\n",
    "#   print(b)\n",
    "\n",
    "reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.6993)\n"
     ]
    }
   ],
   "source": [
    "def tranfer_maps(maps: Tensor):\n",
    "    m = []\n",
    "    maps = torch.squeeze(maps, 0)\n",
    "    uniq, count= maps.unique(return_counts=True)\n",
    "    for i in range(len(count)):\n",
    "        m.append(torch.stack([uniq[i]]*count[i]).tolist())\n",
    "    return m\n",
    "\n",
    "\n",
    "\n",
    "a = reduce_shape_old(last_hidden_state, maps) - reduce_shape_old(last_hidden_state,maps )\n",
    "with torch.no_grad():\n",
    "    \n",
    "    print(torch.mean(a))\n",
    "\n",
    "# tranfer_maps(maps_tensor), maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %reload_ext autoreload\n",
    "from models.layout_parsing import LitBaseParsing\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from models.base import  LitLayoutParsing\n",
    "model = LitBaseParsing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# totalGraph =  model(input_ids, attention_mask, token_type_ids, bbox,maps)\n",
    "\n",
    "# totalGraph.shape\n",
    "dummy_input = torch.zeros(3,126)\n",
    "dummy_input[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dummy_input = torch.zeros_like(input_ids).cuda()\n",
    "# dummy_boxes = torch.zeros_like(bbox).cuda()\n",
    "# dummy_maps = torch.zeros_like(maps).cuda()\n",
    "import torch\n",
    "dummy_input = torch.zeros(1,196).cuda()\n",
    "dummy_boxes = torch.zeros(1,196,4).cuda()\n",
    "dummy_maps = torch.zeros(1,196).cuda()\n",
    "# totalGraph[0].shape\n",
    "# S = totalGraph[0].unsqueeze(0)\n",
    "# G = totalGraph[1].unsqueeze(0)\n",
    "# dummy_maps.shape\n",
    "# dummy_maps.unsqueeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "providers = [\n",
    "            # 'TensorrtExecutionProvider',\n",
    "                     'CUDAExecutionProvider',\n",
    "                     'CPUExecutionProvider']\n",
    "\n",
    "ort_sess = ort.InferenceSession(\n",
    "                                path_or_bytes = 'resources/onnx/LitLP_06-03-2023_18-46-25_fp32.onnx'\n",
    "                                , providers= providers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bbox'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_name = ort_sess.get_inputs()\n",
    "# output_name = ort_sess.get_outputs()[0].name\n",
    "input_name[3].name\n",
    "\n",
    "\n",
    "# outputs = ort_sess.run(None, \n",
    "#                         {\n",
    "#                             'input_1': input_ids.numpy()\n",
    "#                             'input_2': input_ids.numpy()\n",
    "#                             'input_3': input_ids.numpy()\n",
    "#                             'input_4': input_ids.numpy()\n",
    "#                             'input_5': input_ids.numpy()\n",
    "#                         }\n",
    "#                         )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "a = torch.tensor([1, 2, 2, 1,                                                                        \n",
    "1, 1, 2, 1, 2, 2, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 7,                          \n",
    "        1, 1, 1, 2, 1, 2, 2, 5, 2, 3, 1, 3, 2, 1, 1, 2, 1, 3, 3, 2, 2, 2, 3, 2,      \n",
    "        1, 1, 2, 2, 3, 3, 3, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 6, 1, 1, 3, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"dasudlasdjakhihi 'adas'dasoi'\""
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1/a[1].numpy()\n",
    "\n",
    "# for i in range(a[1]):\n",
    "f = 'hihi'\n",
    "tr= f\"dasudlasdjak{f} 'adas'dasoi'\"\n",
    "#     print(i)\n",
    "tr"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
